{"nbformat_minor": 0, "cells": [{"execution_count": null, "cell_type": "code", "source": "%%configure -f \n{ \"numExecutors\":4, \"executorMemory\":\"1G\", \"executorCores\":1, \"driverMemory\":\"1G\", \"driverCores\":1 }", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "case class Student(name: String, subject: String, major: String, school: String, year: Int)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Initialize the data for creating an example Student DataFrame", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import scala.collection.mutable.ListBuffer\n\nval alphabets: Array[Char] = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\".toCharArray()\n\nval nameListBuffer: ListBuffer[String] = new ListBuffer\nval subjectListBuffer: ListBuffer[String] = new ListBuffer\nval majorListBuffer: ListBuffer[String] = new ListBuffer\nval schoolListBuffer: ListBuffer[String] = new ListBuffer\nval yearListBuffer: ListBuffer[Int] = new ListBuffer\n\nfor (alphabet <- alphabets) {\n    \n    nameListBuffer += \"Name\" + alphabet \n}\n\nfor (i <- 0 until 15) {\n    \n    subjectListBuffer += \"Subject\" + i \n}\n\nfor (i <- 0 until 10) {\n    \n    majorListBuffer += \"Major\" + i \n}\n\nfor (i <- 0 until 5) {\n    \n    schoolListBuffer += \"School\" + i \n}\n\nfor (i <- 1 until 5) {\n    \n    yearListBuffer += i\n}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Generate the data for creating the example Student DataFrame", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val randomGenerator = scala.util.Random\n\nval studentListBuffer: ListBuffer[Student] = new ListBuffer\n\nfor (i <- 1 to 100) {\n\n    studentListBuffer += new Student(nameListBuffer(randomGenerator.nextInt(alphabets.size)),\n                                     subjectListBuffer(randomGenerator.nextInt(subjectListBuffer.size)),\n                                     majorListBuffer(randomGenerator.nextInt(majorListBuffer.size)),\n                                     schoolListBuffer(randomGenerator.nextInt(schoolListBuffer.size)),\n                                     yearListBuffer(randomGenerator.nextInt(yearListBuffer.size))\n                                    )\n}", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Create the Student DataFrame", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val students = sqlContext.createDataFrame(studentListBuffer.toList)\n\nstudents.take(5)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Partition DataFrame vertically on one or more column boundaries for caching", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val verticalPartitionCount = 2\n\nval columnCountPerPartition: Int = math.ceil(students.columns.size / verticalPartitionCount).toInt\n\nfor (verticalPartitionIndex <- 0 until verticalPartitionCount + 1) {\n    \n    val startColumnIndex: Int = verticalPartitionIndex * columnCountPerPartition\n    var endColumnIndex: Int = startColumnIndex + columnCountPerPartition\n    \n    endColumnIndex = if (endColumnIndex > students.columns.size) students.columns.size else endColumnIndex\n    \n    val selectedColumns: Array[String] = students.columns.slice(startColumnIndex, endColumnIndex)\n    \n    val verticalPartition = students.select(selectedColumns.head, selectedColumns.tail: _*)\n    \n    verticalPartition.persist()\n    \n    //Alternatively, one can do any other transformation and/or store the output in persistent store\n    \n    for (columnName <- selectedColumns) {\n        \n        verticalPartition.select(columnName).rdd.map((_, 1)).reduceByKey(_ + _)\n        .sortBy(_._2, false)\n        .take(5)\n        .map(println(_))\n    }\n    \n    verticalPartition.unpersist()\n}", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Partition DataFrame vertically on one or more column boundaries and horizontally on one or more row boundaries for caching", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.spark.sql.types.{LongType, StructField, StructType}\nimport org.apache.spark.sql.Row\nimport org.apache.spark.rdd.RDD\n\nval horizontalPartitionCount = 3\n\nval rowCount = students.count\n\nval rowCountPerPartition: Int = math.ceil(rowCount / horizontalPartitionCount).toInt\n\nfor (verticalPartitionIndex <- 0 until verticalPartitionCount + 1) {\n    \n    val startColumnIndex: Int = verticalPartitionIndex * columnCountPerPartition\n    var endColumnIndex: Int = startColumnIndex + columnCountPerPartition\n    \n    endColumnIndex = if (endColumnIndex > students.columns.size) students.columns.size else endColumnIndex\n    \n    val selectedColumns: Array[String] = students.columns.slice(startColumnIndex, endColumnIndex)\n    \n    val verticalPartition = students.select(selectedColumns.head, selectedColumns.tail: _*)\n    \n    val verticalPartitionWithIndex = sqlContext.createDataFrame(verticalPartition.rdd.zipWithIndex().map(\n            r => Row.fromSeq(Seq(r._2) ++ r._1.toSeq)), StructType(\n            Array(StructField(\"index\", LongType, false)) ++ verticalPartition.schema.fields))\n    \n    var columnNameOutputMap : scala.collection.mutable.Map[String, RDD[(Row, Int)]]\n    = scala.collection.mutable.Map[String, RDD[(Row, Int)]]()\n    \n    for (horizontalPartitionIndex <- 0 until horizontalPartitionCount + 1) {\n        \n        val startRowIndex: Long = horizontalPartitionIndex * rowCountPerPartition\n        var endRowIndex: Long = startRowIndex + rowCountPerPartition\n        \n        endRowIndex = if (endRowIndex > rowCount) rowCount else endRowIndex\n        \n        val blockPartition = verticalPartitionWithIndex.filter(f\"index >= $startRowIndex and index < $endRowIndex\")\n        \n        blockPartition.persist()\n    \n        for (columnName <- selectedColumns) {\n        \n            //Alternatively, one can do any other transformation and/or store the output in persistent store\n            \n            if (columnNameOutputMap.contains(columnName)) {\n            \n                columnNameOutputMap(columnName) = columnNameOutputMap(columnName)\n                .union(blockPartition.select(columnName).rdd.map((_, 1)).reduceByKey(_ + _))\n                .reduceByKey(_ + _)\n            \n            } else {\n                \n                columnNameOutputMap += (columnName -> blockPartition.select(columnName).rdd.map((_, 1)).reduceByKey(_ + _))\n            } \n        }\n       \n        blockPartition.unpersist()\n    }\n    \n    for (columnName <- selectedColumns) {\n        \n        columnNameOutputMap(columnName)\n        .sortBy(_._2, false)\n        .take(5)\n        .map(println(_))\n    }\n}", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}