{"nbformat_minor": 0, "cells": [{"execution_count": null, "cell_type": "code", "source": "%%configure -f \n{ \"numExecutors\":4, \"executorMemory\":\"1G\", \"executorCores\":1, \"driverMemory\":\"1G\", \"driverCores\":1 }", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "case class Item(category: String, name: String, price: Double)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "import scala.collection.mutable.ListBuffer\n\nval categoryListBuffer: ListBuffer[String] = new ListBuffer\n\nfor (i <- 1 to 5) {\n    \n    categoryListBuffer += java.util.UUID.randomUUID.toString\n}\n\nval randomGenerator = scala.util.Random\n\nval itemListBuffer: ListBuffer[Item] = new ListBuffer\n\nfor (i <- 1 to 25) {\n\n    itemListBuffer += new Item(categoryListBuffer(randomGenerator.nextInt(5)), f\"Item$i\", randomGenerator.nextDouble() * 100)\n}", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "val items = sqlContext.createDataFrame(itemListBuffer.toList)\n\nitems.take(5)", "outputs": [], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "## <font color='red'>What not to do - Avoid using Collect, Loop and Broadcast</font> ", "cell_type": "markdown", "metadata": {}}, {"source": "### <font color='red'>Collect</font> is a potential bottleneck at large data size", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val uniqueCategoriesList = items.select(\"category\").distinct.collect", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "import scala.collection.mutable.Map\n\nvar indexedUniqueCategoriesMap : Map[String, Long] = Map[String, Long]()\n\nfor (i <- 0 to uniqueCategoriesList.length - 1) {\n    \n    indexedUniqueCategoriesMap += (uniqueCategoriesList(i).get(0).toString -> i)\n}", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### <font color='red'>Broadcast</font> is a potential bottleneck at large data size", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "sc.broadcast(indexedUniqueCategoriesMap)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.spark.sql.functions._\n\nval lookupIndex: (String => Long) = (categoryName: String) => {\n\n    indexedUniqueCategoriesMap.get(categoryName).getOrElse(-1)\n}\n\nval lookupIndexFunction = udf(lookupIndex)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "var indexedItems = items.withColumn(\"index\", lookupIndexFunction(items(\"category\")))\n\nindexedItems.take(5)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## <font color='green'>What to do - Maintain the data always as distributed DataFrame</font>", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import org.apache.spark.sql.types.{LongType, StructField, StructType}\nimport org.apache.spark.sql.{Row, SQLContext}\n\nval uniqueCategories = items.select(\"category\").distinct.withColumnRenamed(\"category\", \"uniquecategory\")\n\nval indexedUniqueCategories = sqlContext.createDataFrame(uniqueCategories.rdd.zipWithIndex().map(\n    r => Row.fromSeq(Seq(r._2) ++ r._1.toSeq)), StructType(\n          Array(StructField(\"index\", LongType, false)) ++ uniqueCategories.schema.fields))\n\nindexedUniqueCategories.take(5)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Method 1 - Using JOIN of two DataFrames", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "indexedItems = items.join(indexedUniqueCategories, items(\"category\")\n                                     === indexedUniqueCategories(\"uniquecategory\")).drop(\"uniquecategory\")\n\nindexedItems.take(5)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Method 2 - Using DataFrame as lookup table in UDF  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "val getIndex: (String => Long) = (categoryName: String) => {\n\n    indexedUniqueCategories.filter(indexedUniqueCategories(\"uniquecategory\")\n                            === categoryName).select(\"index\").first().get(0).asInstanceOf[Long]\n}\n\nval getIndexFunction = udf(getIndex)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "indexedItems = items.withColumn(\"index\", getIndexFunction(items(\"category\")))\n\nindexedItems.take(5)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "val reorderedIndexedItems = indexedItems.select(\"index\", \"category\", \"name\", \"price\")\n\nreorderedIndexedItems.take(5)", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}