{"nbformat_minor": 0, "cells": [{"execution_count": null, "cell_type": "code", "source": "%%configure -f \n{ \"numExecutors\":16, \"executorMemory\":\"4G\", \"executorCores\":1, \"driverMemory\":\"4G\", \"driverCores\":4,\n \"conf\":{\"spark.streaming.receiver.writeAheadLog.enable\":\"true\"}, \n \"jars\": [\"wasb:///powerbi/adal4j-1.1.2.jar\", \"wasb:///powerbi/microsoft-spark-powerbi-connector_2.10-0.6.0.jar\",\n          \"wasb:///streaming/spark-streaming-eventhubs_2.10-1.0.0.jar\"] }", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "// Case class defining event structure\n\ncase class EventContent(EventDetails: String)\n\n// Eventhubs related settings. It is recommended that the number of executors be at least double the number of\n// eventhubs partitions. Number of executors are set in the configure statement above. For this example\n// number of executor count is set at 16 since the number of partitions in the eventhubs used is 8.\n\nval eventhubsNamespace: String = \"***Enter Service Bus namespace here***\"\nval eventhubsName: String = \"***Enter Eventhubs name here***\"\nval eventhubsReceivePolicyName: String = \"***Enter Eventhubs receive policy name***\"\nval eventhubsReceivePolicyKey: String = \"Enter Eventhubs receive policy key***\"\nval eventhubsConsumerGroup: String = \"$default\"\nval eventhubsPartitionCount: String = \"8\"\nval eventhubsCheckpointIntervalInSeconds: String = \"15\"\nval eventhubsCheckpointDirectory: String = \"/EventCheckpoint-15-8-16\"\nval eventhubsDataDirectory: String = \"/EventCount-15-8-16/EventCount15\"\n\n// Streaming related settings\n\nval batchingIntervalInSeconds: Int = 15\nval windowingIntervalInSeconds: Int = 15\nval runDurationInSeconds: Int = 3600\n\n// Powerbi authentication related settings\n\nval powerbiClientId = \"***Enter PowerBI Client Id here***\" \nval powerbiAccountUsername = \"***Enter PowerBI Account Username here***\"\nval powerbiAccountPassword = \"***Enter PowerBI Account Password here***\"\n\n//Powerbi data related settings\n\nval powerbiDatasetName: String = \"Eventhubs Spark Streaming Metrics\"\n", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "//Method to create the streaming context and process the streaming events. The number of events processed per batch (using count)\n//and the cumulative number of events processed since start (using updateStateByKey) are the two metrics sent to PowerBI.   \n\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.apache.spark.streaming.dstream.DStream\nimport org.apache.spark.streaming.eventhubs.EventHubsUtils\n\nimport com.microsoft.spark.powerbi.authentication._\nimport com.microsoft.spark.powerbi.models._\nimport com.microsoft.spark.powerbi.common._\n\ndef createStreamingContext(powerbiDatasetDetails: PowerBIDatasetDetails, powerbiTableList: List[table],\n                           powerBIAuthentication: PowerBIAuthentication): StreamingContext = {\n\n   \n    val streamLengthKey: String = \"StreamLength\"\n\n    val streamLength = (values: Seq[Long], state: Option[Long]) => {\n          val currentCount = values.foldLeft(0L)(_ + _)\n          val previousCount = state.getOrElse(0L)\n          Some(currentCount + previousCount)\n    }\n    \n    val eventHubsParameters = Map[String, String](\n      \"eventhubs.namespace\" -> eventhubsNamespace,\n      \"eventhubs.name\" -> eventhubsName,\n      \"eventhubs.policyname\" -> eventhubsReceivePolicyName,\n      \"eventhubs.policykey\" -> eventhubsReceivePolicyKey,\n      \"eventhubs.consumergroup\" -> eventhubsConsumerGroup,\n      \"eventhubs.partition.count\" -> eventhubsPartitionCount,\n      \"eventhubs.checkpoint.interval\" -> eventhubsCheckpointIntervalInSeconds,\n      \"eventhubs.checkpoint.dir\" -> eventhubsCheckpointDirectory\n    )\n    \n    val streamingContext = new StreamingContext(sc, Seconds(batchingIntervalInSeconds))\n\n    val eventHubsStream = EventHubsUtils.createUnionStream(streamingContext, eventHubsParameters)\n\n    val eventHubsWindowedStream: DStream[Array[Byte]] = eventHubsStream.window(Seconds(windowingIntervalInSeconds))\n\n    import com.microsoft.spark.powerbi.extensions.DStreamExtensions._\n\n    // Save the number of events received per batching interval to PowerB\n    \n    eventHubsWindowedStream.map(m => EventContent(new String(m))).countTimelineToPowerBI(powerbiDatasetDetails,\n        powerbiTableList.head, powerBIAuthentication)\n\n    val batchEventCountDStream = eventHubsWindowedStream.count()\n\n    batchEventCountDStream.print()\n\n    // Count number of events received so far and save to PowerBI and default storage\n\n    import com.microsoft.spark.powerbi.extensions.PairedDStreamExtensions._\n\n    val totalEventCountMappedDStream = eventHubsWindowedStream.map(m => (streamLengthKey, 1L))\n    val totalEventCountDStream: DStream[(String, Long)]\n    = totalEventCountMappedDStream.updateStateByKey[Long](streamLength)\n\n    totalEventCountDStream.stateTimelineToPowerBI(powerbiDatasetDetails, powerbiTableList.last, powerBIAuthentication)\n\n    totalEventCountDStream.print()\n\n    // Save the cumulative count of events\n\n    totalEventCountDStream.saveAsTextFiles(eventhubsDataDirectory)\n    \n    streamingContext.checkpoint(eventhubsCheckpointDirectory)\n\n    streamingContext\n  }", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "//Initialize PowerBI Authentication and create or get PowerBI dataset and table\n\ndef initializeAuthentication(): PowerBIAuthentication = {\n\n    val powerBIAuthentication: PowerBIAuthentication = new PowerBIAuthentication(\n      PowerBIURLs.Authority,\n      PowerBIURLs.Resource,\n      powerbiClientId,\n      powerbiAccountUsername,\n      powerbiAccountPassword\n    )\n\n    powerBIAuthentication\n}\n\nval powerbiAuthentication: PowerBIAuthentication = initializeAuthentication()\n\nval batchEventCountColumns = Map(\"Timestamp\" -> PowerBIDataTypes.DateTime.toString(),\n                                 \"Batch Event Count\" -> PowerBIDataTypes.Int64.toString())\n\nval batchEventCountTable = PowerBIUtils.defineTable(\"BatchEventCountTable\", batchEventCountColumns)\n\nval cumulativeEventCountColumns = Map(\"Timestamp\" -> PowerBIDataTypes.DateTime.toString(),\n                                      \"Cumulative Event Count\" -> PowerBIDataTypes.Int64.toString())\n\nval cumulativeEventCountTable = PowerBIUtils.defineTable(\"CumulativeEventCountTable\", cumulativeEventCountColumns)\n\nval powerbiTableList = List[table](batchEventCountTable, cumulativeEventCountTable)\n\nval powerbiDatasetDetails = PowerBIUtils.getOrCreateDataset(powerbiDatasetName, powerbiTableList, PowerBIOptions.basicFIFO,\n      powerbiAuthentication.getAccessToken())\n\nprintln(powerbiDatasetDetails)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": " //Create or recreate streaming context using the same checkpoint directory specified as Eventhubs receiver parameter\n\nval streamingContext = StreamingContext.getOrCreate(eventhubsCheckpointDirectory, () => createStreamingContext(powerbiDatasetDetails, powerbiTableList, powerbiAuthentication))\n\nstreamingContext.start()\n\nstreamingContext.awaitTerminationOrTimeout(runDurationInSeconds * 1000)", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}