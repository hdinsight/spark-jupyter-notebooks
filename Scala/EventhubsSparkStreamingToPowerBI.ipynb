{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%configure -f { \"numExecutors\":16, \"executorMemory\":\"4G\", \"executorCores\":1, \"driverMemory\":\"4G\", \"driverCores\":4, \"conf\":{\"spark.streaming.receiver.writeAheadLog.enable\":\"true\"}, \"jars\": [\"wasb:///example/jars/adal4j-1.1.2.jar\", \"wasb:///example/jars/microsoft-spark-powerbi-connector.jar\", \"wasb:///example/jars/spark-streaming-eventhubs-example-1.6.0.2.4.1.0-327-jar-with-dependencies.jar\"] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "// Case class defining event structure\n",
    "\n",
    "case class EventContent(EventDetails: String)\n",
    "\n",
    "// Eventhubs related settings. It is recommended that the number of executors be at least double the number of\n",
    "// eventhubs partitions. Number of executors are set in the configure statement above. For this example\n",
    "// number of executor count is set at 16 since the number of partitions in the eventhubs used is 8.\n",
    "\n",
    "val eventhubsNamespace: String = \"***Enter Service Bus namespace here***\"\n",
    "val eventhubsName: String = \"***Enter Eventhubs name here***\"\n",
    "val eventhubsReceivePolicyName: String = \"***Enter Eventhubs receive policy name***\"\n",
    "val eventhubsReceivePolicyKey: String = \"Enter Eventhubs receive policy key***\"\n",
    "val eventhubsConsumerGroup: String = \"$default\"\n",
    "val eventhubsPartitionCount: String = \"8\"\n",
    "val eventhubsCheckpointIntervalInSeconds: String = \"15\"\n",
    "val eventhubsCheckpointDirectory: String = \"/EventCheckpoint-15-8-16\"\n",
    "val eventhubsDataDirectory: String = \"/EventCount-15-8-16/EventCount15\"\n",
    "\n",
    "// Streaming related settings\n",
    "\n",
    "val batchingIntervalInSeconds: Int = 15\n",
    "val windowingIntervalInSeconds: Int = 15\n",
    "val runDurationInSeconds: Int = 3600\n",
    "\n",
    "// Powerbi authentication related settings\n",
    "\n",
    "val powerbiClientId = \"***Enter PowerBI Client Id here***\" \n",
    "val powerbiAccountUsername = \"***Enter PowerBI Account Username here***\"\n",
    "val powerbiAccountPassword = \"***Enter PowerBI Account Password here***\"\n",
    "\n",
    "//Powerbi data related settings\n",
    "\n",
    "val powerbiDatasetName: String = \"Eventhubs Spark Streaming Metrics\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Method to create the streaming context and process the streaming events. The number of events processed per batch (using count)\n",
    "//and the cumulative number of events processed since start (using updateStateByKey) are the two metrics sent to PowerBI.   \n",
    "\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import org.apache.spark.streaming.dstream.DStream\n",
    "import org.apache.spark.streaming.eventhubs.EventHubsUtils\n",
    "\n",
    "import com.microsoft.spark.powerbi.authentication._\n",
    "import com.microsoft.spark.powerbi.models._\n",
    "import com.microsoft.spark.powerbi.common._\n",
    "\n",
    "def createStreamingContext(powerbiDatasetDetails: PowerBIDatasetDetails, powerbiTableList: List[table],\n",
    "                           powerBIAuthentication: PowerBIAuthentication): StreamingContext = {\n",
    "\n",
    "   \n",
    "    val streamLengthKey: String = \"StreamLength\"\n",
    "\n",
    "    val streamLength = (values: Seq[Long], state: Option[Long]) => {\n",
    "          val currentCount = values.foldLeft(0L)(_ + _)\n",
    "          val previousCount = state.getOrElse(0L)\n",
    "          Some(currentCount + previousCount)\n",
    "    }\n",
    "    \n",
    "    val eventHubsParameters = Map[String, String](\n",
    "      \"eventhubs.namespace\" -> eventhubsNamespace,\n",
    "      \"eventhubs.name\" -> eventhubsName,\n",
    "      \"eventhubs.policyname\" -> eventhubsReceivePolicyName,\n",
    "      \"eventhubs.policykey\" -> eventhubsReceivePolicyKey,\n",
    "      \"eventhubs.consumergroup\" -> eventhubsConsumerGroup,\n",
    "      \"eventhubs.partition.count\" -> eventhubsPartitionCount,\n",
    "      \"eventhubs.checkpoint.interval\" -> eventhubsCheckpointIntervalInSeconds,\n",
    "      \"eventhubs.checkpoint.dir\" -> eventhubsCheckpointDirectory\n",
    "    )\n",
    "    \n",
    "    val streamingContext = new StreamingContext(sc, Seconds(batchingIntervalInSeconds))\n",
    "\n",
    "    val eventHubsStream = EventHubsUtils.createUnionStream(streamingContext, eventHubsParameters)\n",
    "\n",
    "    val eventHubsWindowedStream: DStream[Array[Byte]] = eventHubsStream.window(Seconds(windowingIntervalInSeconds))\n",
    "\n",
    "    import com.microsoft.spark.powerbi.extensions.DStreamExtensions._\n",
    "\n",
    "    // Save the number of events received per batching interval to PowerB\n",
    "    \n",
    "    eventHubsWindowedStream.map(m => EventContent(new String(m))).countTimelineToPowerBI(powerbiDatasetDetails,\n",
    "        powerbiTableList.head, powerBIAuthentication)\n",
    "\n",
    "    val batchEventCountDStream = eventHubsWindowedStream.count()\n",
    "\n",
    "    batchEventCountDStream.print()\n",
    "\n",
    "    // Count number of events received so far and save to PowerBI and default storage\n",
    "\n",
    "    import com.microsoft.spark.powerbi.extensions.PairedDStreamExtensions._\n",
    "\n",
    "    val totalEventCountMappedDStream = eventHubsWindowedStream.map(m => (streamLengthKey, 1L))\n",
    "    val totalEventCountDStream: DStream[(String, Long)]\n",
    "    = totalEventCountMappedDStream.updateStateByKey[Long](streamLength)\n",
    "\n",
    "    totalEventCountDStream.stateTimelineToPowerBI(powerbiDatasetDetails, powerbiTableList.last, powerBIAuthentication)\n",
    "\n",
    "    totalEventCountDStream.print()\n",
    "\n",
    "    // Save the cumulative count of events\n",
    "\n",
    "    totalEventCountDStream.saveAsTextFiles(eventhubsDataDirectory)\n",
    "    \n",
    "    streamingContext.checkpoint(eventhubsCheckpointDirectory)\n",
    "\n",
    "    streamingContext\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Initialize PowerBI Authentication and create or get PowerBI dataset and table\n",
    "\n",
    "def initializeAuthentication(): PowerBIAuthentication = {\n",
    "\n",
    "    val powerBIAuthentication: PowerBIAuthentication = new PowerBIAuthentication(\n",
    "      PowerBIURLs.Authority,\n",
    "      PowerBIURLs.Resource,\n",
    "      powerbiClientId,\n",
    "      powerbiAccountUsername,\n",
    "      powerbiAccountPassword\n",
    "    )\n",
    "\n",
    "    powerBIAuthentication\n",
    "}\n",
    "\n",
    "val powerbiAuthentication: PowerBIAuthentication = initializeAuthentication()\n",
    "\n",
    "val batchEventCountColumns = Map(\"Timestamp\" -> PowerBIDataTypes.DateTime.toString(),\n",
    "                                 \"Batch Event Count\" -> PowerBIDataTypes.Int64.toString())\n",
    "\n",
    "val batchEventCountTable = PowerBIUtils.defineTable(\"BatchEventCountTable\", batchEventCountColumns)\n",
    "\n",
    "val cumulativeEventCountColumns = Map(\"Timestamp\" -> PowerBIDataTypes.DateTime.toString(),\n",
    "                                      \"Cumulative Event Count\" -> PowerBIDataTypes.Int64.toString())\n",
    "\n",
    "val cumulativeEventCountTable = PowerBIUtils.defineTable(\"CumulativeEventCountTable\", cumulativeEventCountColumns)\n",
    "\n",
    "val powerbiTableList = List[table](batchEventCountTable, cumulativeEventCountTable)\n",
    "\n",
    "val powerbiDatasetDetails = PowerBIUtils.getOrCreateDataset(powerbiDatasetName, powerbiTableList, PowerBIOptions.basicFIFO,\n",
    "      powerbiAuthentication.getAccessToken())\n",
    "\n",
    "println(powerbiDatasetDetails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " //Create or recreate streaming context using the same checkpoint directory specified as Eventhubs receiver parameter\n",
    "\n",
    "val streamingContext = StreamingContext.getOrCreate(eventhubsCheckpointDirectory, () => createStreamingContext(powerbiDatasetDetails, powerbiTableList, powerbiAuthentication))\n",
    "\n",
    "streamingContext.start()\n",
    "\n",
    "streamingContext.awaitTerminationOrTimeout(runDurationInSeconds * 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
